# -*- coding: utf-8 -*-
"""CodeInterpreter_ChatGPT_IRIS_Explore.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15R_ofVOWXIajBqNMMeobzY9LhYx7XarW

# Load the data in from Google Drive
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd

# Reload the CSV file into a DataFrame with correct column names
iris_df = pd.read_csv('/content/gdrive/MyDrive/PGSS2022/PGSS2023/Week01/iris.csv',
                      names=["Petal.Length", "Petal.Width", "Sepal.Length", "Sepal.Width", "Species"])

# Display the first few rows of the DataFrame
iris_df.head()

"""# Exploratory Data Analysis"""

# Display basic statistics
basic_stats = iris_df.describe(include='all')

# Count the number of instances for each species
species_counts = iris_df["Species"].value_counts()

basic_stats, species_counts

import seaborn as sns
import matplotlib.pyplot as plt

# Create pairplot to visualize the distributions and relationships between different features for each species
sns.pairplot(iris_df, hue="Species")
plt.show()

"""# Hypothesis Exploration using Statistical Testing , for Petal Length"""

# Explore Petal Length Hypothesis: The average petal length differs significantly between the species.
# import statsmodels.api         as sm
# from statsmodels.formula.api   import ols
# mod_f_t = ols(formula='Petal.Length ~ Sepal.Width + Sepal.Length + Petal.Width + Species', data = iris_df)
# res_f_t = mod_f_t.fit()
# print(res_f_t.summary())

import pandas as pd
from scipy.stats import shapiro, levene, f_oneway

# Load the CSV file into a DataFrame
# iris_df = pd.read_csv('iris.csv', names=["Petal.Length", "Petal.Width", "Sepal.Length", "Sepal.Width", "Species"])

# Check normality
_, p_shapiro_setosa = shapiro(iris_df[iris_df['Species'] == 'Iris-setosa']['Petal.Length'])
_, p_shapiro_versicolor = shapiro(iris_df[iris_df['Species'] == 'Iris-versicolor']['Petal.Length'])
_, p_shapiro_virginica = shapiro(iris_df[iris_df['Species'] == 'Iris-virginica']['Petal.Length'])


# Log transform the continuous variable to help ensure homoscedacticity
import numpy as np
iris_df['Petal.Length.Log'] = np.log(iris_df['Petal.Length'])

# Check homogeneity of variances
_, p_levene = levene(
    iris_df[iris_df['Species'] == 'Iris-setosa']['Petal.Length'],
    iris_df[iris_df['Species'] == 'Iris-versicolor']['Petal.Length'],
    iris_df[iris_df['Species'] == 'Iris-virginica']['Petal.Length']
)

# Perform ANOVA
_, p_anova = f_oneway(
    iris_df[iris_df['Species'] == 'Iris-setosa']['Petal.Length'],
    iris_df[iris_df['Species'] == 'Iris-versicolor']['Petal.Length'],
    iris_df[iris_df['Species'] == 'Iris-virginica']['Petal.Length']
)

p_shapiro_setosa, p_shapiro_versicolor, p_shapiro_virginica, p_levene, p_anova

from scipy.stats import mannwhitneyu

# Perform Mann-Whitney U test for each pair of species
_, p_mannwhitneyu_setosa_versicolor = mannwhitneyu(
    iris_df[iris_df['Species'] == 'Iris-setosa']['Petal.Length'],
    iris_df[iris_df['Species'] == 'Iris-versicolor']['Petal.Length']
)

_, p_mannwhitneyu_setosa_virginica = mannwhitneyu(
    iris_df[iris_df['Species'] == 'Iris-setosa']['Petal.Length'],
    iris_df[iris_df['Species'] == 'Iris-virginica']['Petal.Length']
)

_, p_mannwhitneyu_versicolor_virginica = mannwhitneyu(
    iris_df[iris_df['Species'] == 'Iris-versicolor']['Petal.Length'],
    iris_df[iris_df['Species'] == 'Iris-virginica']['Petal.Length']
)

p_mannwhitneyu_setosa_versicolor, p_mannwhitneyu_setosa_virginica, p_mannwhitneyu_versicolor_virginica

"""# Lets try and see how often we are correct at classifying Species using this statistically significant differentiator of species"""

# Build a decision tree classifier for Species using Petal.Length as a feature
from sklearn.preprocessing  import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree            import DecisionTreeClassifier
from sklearn import metrics
x = iris_df[['Petal.Length']]
y = iris_df['Species']
labelencoder = LabelEncoder()
y = labelencoder.fit_transform(y)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1, test_size=0.5)
entropyClf=DecisionTreeClassifier(criterion="entropy",  max_depth=3)
entropyClf.fit(x_train,y_train)

# Use dtreeviz to visualize the decision rules embodied by entropyClf

y_pred=entropyClf.predict(x_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

# !pip install dtreeviz

# from dtreeviz.trees import *

# viz = dtreeviz(entropyClf,
#                 x_train,
#                 y_train,
#                 target_name='Species',
#                 feature_names=x.columns,
#                 class_names=list(labelencoder.classes_)
#                 )

# viz.view()

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(12,8))
plot_tree(entropyClf, filled=True, feature_names=['Petal.Length'], class_names=labelencoder.classes_)
plt.show()

# 5 fold cross validation result
from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation
scores = cross_val_score(entropyClf, x, y, cv=5)

# Compute the mean accuracy
mean_accuracy = scores.mean()

mean_accuracy

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

# Generate cross-validated predictions for each data point
y_pred = cross_val_predict(entropyClf, x, y, cv=5)

# Print a classification report showing precision, recall, and F1-score per class
print(classification_report(y, y_pred, target_names=labelencoder.classes_))

"""# Explore what this classification 5-fold CV report looks like for the other 3 continuous dependent variables for the Species response"""



"""# Explore what the report looks like if you use ALL 4 as dependent variables predicting the Species Response"""

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import classification_report

x = iris_df[['Petal.Length','Petal.Width', 'Sepal.Length','Sepal.Width']]
y = iris_df['Species']

# Generate cross-validated predictions for each data point
y_pred = cross_val_predict(entropyClf, x, y, cv=5)

# Print a classification report showing precision, recall, and F1-score per class
print(classification_report(y, y_pred, target_names=labelencoder.classes_))